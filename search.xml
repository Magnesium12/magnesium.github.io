<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[数据结构]]></title>
    <url>%2F2018%2F12%2F13%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%2F</url>
    <content type="text"><![CDATA[谨以此文, 纪念我没怎么听的数据结构]]></content>
      <categories>
        <category>课程</category>
      </categories>
      <tags>
        <tag>课程笔记</tag>
        <tag>自学OTZ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tutorial For Hexo]]></title>
    <url>%2F2018%2F12%2F12%2FSimple-Tutorial-For-Hexo%2F</url>
    <content type="text"><![CDATA[Hexo是一款基于Node.js的静态博客框架，可以方便的生成静态网页托管在GitHub和Heroku上，是搭建博客的首选框架。Hexo同时也是GitHub上的开源项目，如果想要更加全面的了解Hexo，可以到其官网 Hexo 了解更多的细节，因为Hexo的创建者是台湾人，对中文的支持很友好，你可以查阅到优质的中文文档. Hexo基础基于Github pages 的静态博客所必须的步骤 GitHubGitHub提供了一个免费账户, 这对于我们来说可以很方便的搭建静态博客, 当然, 这也是本篇的目标.登录到GitHub, 如果没有GitHub帐号，使用你的邮箱注册GitHub帐 号, 点击GitHub中的New repository创建新仓库，仓库名应该为：用户名.http://github.io 这个用户名使用你的GitHub帐号名称代替，这是固定写法，比如我的仓库名为：magnesium12.github.io. 安装Git&amp;Node.js在安装前，你必须检查电脑中是否已安装下列应用程序： Node.js Git Windows 用户 如果你的电脑中尚未安装所需要的程序，请到官网下载Node.js &amp; Git的安装程序进行安装.注意安装Node.js时注意勾选Add to PATH选项, 这将使得Node.js添加到你的计算机环境变量中.Git安装完成后，右键菜单会出现git bash, 我们所有的命令都需要在git bash中执行,. 下载Git可能会有些慢, 请耐心等待或者自行寻找过其他源. 配置SSH免密访问 本地安装 Git 我本地是Windows，采用 git-bash. 如果你是新手的话, 在安装过程中只要一路next就OK 配置本地免密ssh远程登陆 在本地写作的机器上，搜索Git Bash，设置user.name和user.email配置信息：12git config --global user.name "你的GitHub用户名"git config --global user.email "你的GitHub注册邮箱" 生成ssh密钥文件1ssh-keygen -t rsa -C "你的GitHub注册邮箱" 一路回车，~/.ssh/目录下会生成 id_rsa 和 id_rsa.pub 两个文件。 打开GitHub_Settings_keys 页面，新建new SSH Key Title为标题，任意填即可，将刚刚复制的id_rsa.pub内容粘贴进去，最后点击Add SSH key.在Git Bash中检测GitHub公钥设置是否成功，输入1​ ssh git@github.com 如果是第一次链接的话, 可能会询问是否创建 known_hosts文件, 当然是yes 建站如果你的电脑中已经安装上述必备程序，那么恭喜你！接下来只需使用 npm 即可完成 Hexo 的安装。 1$ npm install -g hexo-cli Hexo安装完成后, 输入以下命令并执行 123$ hexo init &lt;folder&gt;$ cd &lt;folder&gt;$ npm install 请注意, &lt;folder&gt;是你指定的文件夹路径, 并且该文件夹必须为空. &lt;`&gt;`是特殊字符, 命令端输入时应当去除, 例如 123hexo init /d/Project/Hexocd /d/Project/Hexonpm install 或者直接打开Hexo文件夹, 右键git bash, 输入1hexo init 如果你看到1INFO Start blogging with Hexo!` 这意味着本地建站已完成, congratulations!你已经到达新手村. 站点结构新建完成后, 本地hexo站点结构如下 12345678.├── _config.yml├── package.json├── scaffolds├── source| ├── _drafts| └── _posts└── themes _config.yml网站的配置信息, 你可以在这里选择配置大部分参数, 我们称为站点配置文件. 可以设定主题, 网站标题, 副标题, author etc. 详情请阅读.md文档 package.json应用程序的信息. scaffolds模板文件夹, Hexo会根据scaffolds来建立文件. Hexo的模板是指在新建的markdown文件中默认填充的内容. 例如, 如果您修改scaffold/post.md中的Front-matter内容, 那么每次新建一篇文章时都会包含这个修改. source资源文件夹是存放用户资源的地方. 除_posts 文件夹之外, 开头命名为 _ (下划线)的文件 / 文件夹和隐藏的文件将会被忽略. Markdown 和 HTML 文件会被解析并放到 public 文件夹, 而其他文件会被拷贝过去. 发布的博文存储在/public/ theme主题 文件夹。Hexo 会根据主题来生成静态页面。 存在默认主题, 但还是请选择一个你喜欢的主题, it does matter. 主题文件夹内会有_config.yml配置文件, 我们称为主题配置文件 Hexo常用指令**请注意, 这些操作必须在../hexo/目录下进行 init$ hexo init [folder] 新建一个网站。如果没有设置 folder , Hexo 默认在目前的文件夹建立网站. new$ hexo new [layout] &lt;title&gt; $ hexo n &quot;article&quot; 新建一篇文章。如果没有设置 layout 的话，默认使用 _config.yml 中的 default_layout 参数代替。如果标题包含空格的话，请使用引号括起来。 generate$ hexo generate $ hexo g 生成静态文件。 deploy$ hexo deploy $ hexo d 文件生成后立即部署网站 generate$ hexo gengerate $ hexo g server$ hexo server $ hexo s 启动服务器。默认情况下，访问网址为： http://localhost:4000/ . 如果需要修改,​ $ hexo sever -p 5000 #更改端口至5000 clean$ hexo clean 清除缓存文件 (db.json) 和已生成的静态文件 (public)。在某些情况（尤其是更换主题后）, 如果发现您对站点的更改无论如何也不生效, 您可能需要运行该命令. 发布网站如果你已经迫不及待地打开在hexo 目录下打开Git bash12hexo ghexo s 在http://localhost:4000 你将预览到自己的博客, 但是别人是看不到的, 毕竟一般情况下别人不能看到你的计算机的本地内容. Github pages可以帮助我们解决这个问题. 下一步把GitHub和我们的Hexo关联起来, 打开站点的_config.yml, 找到1234deploy: type: repo: branch: 修改如下:12345deploy: type: git #创建仓库的完整路径, 末尾记得加.git repo: https://github.com/Magnesium12/magnesium12.github.io.git branch: master 其实就是给hexo d 这个命令做相应的配置，让hexo知道你要把blog部署在哪个位置，很显然，我们部署在我们GitHub的仓库里。最后安装Git部署插件, 输入命令: npm install hexo-deployer-git --save 现在我们输入:123hexo cleanhexo ghexo d 在浏览器中输入: xxxx. github.io 你会发现这时你的博客已经上线, Congratulations! Sharing your fantasy to the world.]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>施工中</tag>
        <tag>021</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python爬虫（三）：从JSON解决Ajax]]></title>
    <url>%2F2018%2F12%2F07%2FPython%E7%88%AC%E8%99%AB%EF%BC%88%E4%B8%89%EF%BC%89%EF%BC%9A%E7%88%AC%E5%8F%96js%E5%8A%A8%E6%80%81%E7%BD%91%E9%A1%B5%2F</url>
    <content type="text"><![CDATA[上一篇博文中, 笔者采用了selenium驱动chrome来抓取网页, 总结来看及其低效….. 想了一下, 不能这样搞, 他要刷新局部界面肯定是要利用JavaScript调用已经写好的json的. 通常来说不会有前端选择瞎搞地址. 如果能观察出那个json的地址特点, 就到了ButifulSoup为所欲为的时间辣(●’◡’●) 对于json参数偏移量不规律的网页,那当然是 直接打死 ,哦不, 仔细观察. 对于一个正常的前端同学来说, 不规律的json只可能是有限的, 我不做人啦,jojo!. 我们只要把有限的偏移量写成一个字典(dic)就大功告成 前期准备 Chrome浏览器 Python3.7 requests, BeautifulSoup4模块 一点点耐心以及正常的视力 流程分析 chrome浏览器打开目标网页, 检查元素 F12, CTRL+ shift+i, CTRL+shift+c爱用那个用哪个 继续加载内容，注意观察network栏下的xhr文件 很明显，那个since=就是我们要找的*点击展开，根据 Query string 构造parameter参数字典 requests 负责请求，bs4 为所欲为 代码实现12345678910111213141516171819202122json_url = 'https://富强民主文明和谐'#无规则偏移量，自行逆向分析得到偏移量字典since_list = [26961.554, 26961.474, 26961.419, 26961.346]#实现提取json，加载动态内容，但是封装性不好，不能重复利用def getMorePages(list,count=1): params_list = [] for i in range(count): keyword=str(list[i]) params_list.append(&#123; 'since': keyword, 'grid_type': 'flow', 'sort': 'hot', 'tag_id': '399', &#125;) url = json_url for i in params_list: req=requests.get(url, params=i).content soup=BeautifulSoup(req,'lxml') getURL(soup) 尾声很显然，这样做效率会高很多 发现json规律成为了木桶的最短板，为了避免那个网站被薅羊毛，我就手动屏蔽惹OTZ……另外，代码只放入了相关的一部分，有兴趣的同学可以尝试结合上一篇，整合功能鸭~Good night！]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Web Crawler</tag>
        <tag>实例</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[推歌：A LETTER]]></title>
    <url>%2F2018%2F12%2F03%2F%E6%AD%8C%E6%9B%B2%E6%8E%A8%E8%8D%90The-Song-For-You%2F</url>
    <content type="text"><![CDATA[You don’t have to throw your life away 本来也不想写文字，但是不写一点文字,又怎么知道当时所思所想是什么，自己又是为什么要去写这篇博文呢? …… A LETTER 是由泽野弘之创作的插曲——也许提及高达UC会让他更容易被想起. 歌姬Cyua确实很适合去唱这首歌, 她的声线与舒缓空旷的乐音彼此交融, 编织出这首弥漫着伤感, 空灵以及希望的歌曲. 倘若你愿意闭上双眸, 浮现出的场景大概会是独角兽在孤独地进行着只属于自己的宇宙漫途, 但或许你需要知道的是, 这首歌曲更多在表达的是对自己的鼓励和对未来的希冀. 每个人都是宇宙中的独角兽, 命运指定的航线终将我们彼此分离, 忙碌追赶又有什么意义呢？命运的轨迹充满了错误, 等待我们的会是痛苦, 遗憾, 还是……希望? 指向正确的道路或许很艰辛, 可是总有一天能够到达尽头. Now you see light in pain. Kevin .]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>私货</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python爬虫（二）：Selenium + ChromeDriver 解决异步加载]]></title>
    <url>%2F2018%2F12%2F01%2FPython%E7%88%AC%E8%99%AB%EF%BC%88%E4%BA%8C%EF%BC%89%EF%BC%9ASelenium%20%2B%20ChromeDriver%20%E8%A7%A3%E5%86%B3%E5%BC%82%E6%AD%A5%E5%8A%A0%E8%BD%BD%2F</url>
    <content type="text"><![CDATA[在上一篇博文中, python代码趋向于平铺直叙——或者说代码习惯十分不好, 稍微复杂点的功能都会举步维艰 对于选择使用js控制加载网页结构的网站, 以urllib为基础的python库来说无法直接解决这个问题，例如爬取下拉刷新的网页：”https://bcy.net/coser&quot; 但是事在人为, 对于笔者这个小白来说还是有傻瓜式替代解决思路的, 虽然很慢== 前期准备 默认看过Python爬虫（一）：Requests&amp;BS4 爬虫实例 预装模块: requests, selenium, bs4, os 下载chromedriver 流程分析 bs4+selenium+chromedriver 强行爆破 模拟点击行为获取完全加载的html 然后用beautifulsoup为所欲为代码实现 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117# coding=utf-8# **************************Declaration**************************# @File name: Crawler# @Function: requests+selenium+chromedriver# @Author: Ogiso Kazusa# @Date: 2018/11/15# @Version Number: 2.0# ******************************end******************************import requestsimport osfrom bs4 import BeautifulSoupfrom selenium import webdriver#常量URL = 'https://bcy.net'index_url = 'https://bcy.net/coser'CachePath = "D:\\CrawlerCache\\"#储存CN的键值对, 用于isInDic()dic=&#123;'test':0&#125;#检查字典, 判断是否下载过def isInDic(src): if src in dic: dic[src] = dic[src] + 1 else: dic[src] = 0 dst = src + "_" + str(dic[src]) return dstdef mkdir(path): # 去除首位空格 path = path.strip() # 去除尾部 \ 符号 path = path.rstrip("\\") # 判断路径是否存在 # 存在 True # 不存在 False isExists = os.path.exists(path) # 判断结果 if not isExists: # 如果不存在则创建目录 # 创建目录操作函数 os.makedirs(path) print(path + ' 创建成功') return True else: # 如果目录存在则不创建，并提示目录已存在 print(path + ' 目录已存在') return False# 实现图片下载功能def downloadImg(url, name): # 请求url页面内容,此时页面为图床页面,只有图片内容 url = requests.get(url) # 格式和下载位置 path = CachePath + name + ".jpg" # 迭代器和生成器,实现下载 with open(path, 'wb') as f: f.write(url.content) f.close()#驱动chrome,获取加载完全的html#木桶效应的最短板, 也是本次解决方案的致命点def getJsHtml(URL, cosImg): driver = webdriver.Chrome() driver.get(URL) html = driver.page_source driver.close() soup = BeautifulSoup(html, 'lxml') content = soup.find_all('div', &#123;"class": "img-wrap-inner"&#125;) i = 1 for element in content: src = element.find('img')['src'] name=cosImg + '_' + str(i) downloadImg(src, name) print("下载进度：",i) i = i + 1# 确定存储目录CachePath,没有则生成mkdir(CachePath)# 请求页面内容session = requests.get(index_url) # 获取requests 对象的内容，建议使用content，requests会尝试提供字节数soup = BeautifulSoup(session.content, "lxml") # find 直接返回值, findall 返回列表(list)index = soup.find_all('a', &#123;'class': "db posr ovf"&#125;)for element in index: # print(element)可以看到 # 利用标签的父子关系，能够选择标签或者子标签中的属性值 # 我们获取cos发布地址 img_url = element['href'] url = URL + img_url # CN = element.get('title')貌似两种写法均可 CN = element['title'] print('准备下载：' + CN) CN = isInDic(CN) getJsHtml(url, CN)#大功告成#可以考虑指定需要获取的json？ 尾声 吐槽一下, 这玩意儿奇慢无比, 调用浏览器中出现了大量不必要的步骤== 另一种意义上的模拟人类点击机制, 慢也有点用?,效率实在太低 令人头皮发麻，]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Web Crawler</tag>
        <tag>实例</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python爬虫（一）:Requests&BS4 爬虫实例]]></title>
    <url>%2F2018%2F12%2F01%2FPython%E7%88%AC%E8%99%AB%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9ARequests%26BS4%20%E7%88%AC%E8%99%AB%E5%AE%9E%E4%BE%8B%2F</url>
    <content type="text"><![CDATA[这篇文章主要是方便入门爬虫的同学获得入门的正反馈，配合代码注释可能轻松愉悦的对爬虫有个大概了解~ 我们选择对于使用静态网页的小说网站, 小说的文本内容往往分配在一个&lt;div里, 逻辑比较简单. 那就开始吧＜（＾－＾）＞ 前期准备 安装python3.7 安装requests, BeautifulSoup4(或者说bs4) 安装chrome浏览器 掌握Python基础语法 可以尝试 菜鸟教程-Python 3 教程 流程分析 请求URL指向的页面-&gt;获取网页内容 设定筛选条件-&gt;获取指定内容 写入本地文件 检查网站代码 ctrl +shift +c, 检查网页源代码[^3], 得知章节地址F12, ctrl+shift+i , 右键-检查, 等等均可 [^3]:本教程仅供参考, 保护创作者版权, 提倡付费阅读 根据章节地址跳转页面 检查章节页面, 得知章节文本内容 代码实现1234567891011121314151617181920212223242526272829303132333435363738394041424344454647# coding=utf-8# *********************Declaration*********************# @File name: WebCrawler# @Function: Download Single Novel# @Author: Ogiso Kazusa # @Date: 2018/11/14 # @Version Number: 1.0 # *************************end*************************#导入模块import requestsfrom bs4 import BeautifulSoup#小说网页目录地址, 准备遍历全部章节地址index_url='https://www.88dush.com/xiaoshuo/27/27584/'#获取页面内容index_req=requests.get(index_url)index_html=index_req.contentpage_main=BeautifulSoup(index_html,"lxml")#创建D盘根目录文件“单本下载.txt”，#格式为ab+:向二进制文件末添加数据，且允许读；fo=open("D:\\单本下载.txt","ab+")#获取div , class="mulu" 包含的内容chap_BS=page_main.find("div",&#123;"class":"mulu"&#125;)#生成器对象for child in chap_BS.ul.children: if child!="\n": #href：目标url的属性名 chap_url=index_url+child.a.get("href") #素质三联，获取lxml文档 chap_req=requests.get(chap_url) chap_html=chap_req.content.decode("gbk") soup_text=BeautifulSoup(chap_html,"lxml") #寻找div段落，class=yd_text2的属性块内容 chap_text=soup_text.find("div"，&#123;"class":"yd_text2"&#125;) #.text指获取文字内容，\r\n是指写入文件中的换行符, == fo.write((child.string+ "\r\n"+chap_text.text+"\r\n").encode('utf-8')) print(child.string+'\t已下载') fo.close() 最后Enjoy it !]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Web Crawler</tag>
        <tag>实例</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[A Whisper To The World]]></title>
    <url>%2F2018%2F12%2F01%2FA-Whisper-To-The-World%2F</url>
    <content type="text"><![CDATA[NieR:Automata Everything that lives is designed to end.一切活着的事物，都注定要终结。We are perpetually trapped …我们被永远地囚禁……… in a never-ending spiral of life and death.……于永无止境的生死轮回之中。Is this a curse?这是一种诅咒？Or some kind of punishment?还是某种惩罚？I often think about the god who blessed us with this cryptic puzzle …我时常想起那用模糊的谜团祝福我们的神……… and wonder if we’ll ever have the chance to kill him.……并猜想我们是否终有弑神的机会。]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>Ridiculous</tag>
        <tag>Thought</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2018%2F11%2F30%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
      <tags>
        <tag>hello</tag>
      </tags>
  </entry>
</search>
